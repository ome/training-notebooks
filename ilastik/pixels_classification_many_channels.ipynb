{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze data using ilastik\n",
    "The notebook shows how load images contained in a dataset\n",
    "from OMERO as numpy arrays and analyze them in ilastik.\n",
    "It assumes that ilastik project is linked to the dataset.\n",
    "The 5D-numpy array is in the order expected by the ilastik project.\n",
    "The order might need to be adjusted depending on the ilastik project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "import subprocess\n",
    "from itertools import product\n",
    "# from functools import reduce\n",
    "# from operator import mul\n",
    "\n",
    "import tempfile\n",
    "import omero.clients\n",
    "from omero.gateway import BlitzGateway\n",
    "import omero\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a connection to an OMERO server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HOST = 'localhost'\n",
    "# HOST = 'demo.openmicroscopy.org'\n",
    "PORT = 4064\n",
    "\n",
    "conn = BlitzGateway('facility_staff_1',\n",
    "                    'facility_staff_1_pw',\n",
    "                    host=HOST, port=PORT)\n",
    "\n",
    "# conn = BlitzGateway(input(\"Username: \"),\n",
    "#                     getpass(\"OMERO Password: \"),\n",
    "#                     host=HOST, port=PORT)\n",
    "conn.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter the dataset ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Local server\n",
    "dataset_id = 151\n",
    "\n",
    "# Demo server\n",
    "# dataset_id = 3248\n",
    "training_dataset_tag = 'training_dataset'\n",
    "validation_dataset_tag = 'validation_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function: Load the ilastik project linked to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def load_model(dataset_id, path):\n",
    "    dataset = conn.getObject(\"Dataset\", dataset_id)\n",
    "    file_paths = list()\n",
    "    # Go through all the annotations on the Dataset\n",
    "    for ann in dataset.listAnnotations():\n",
    "        if isinstance(ann, omero.gateway.FileAnnotationWrapper):\n",
    "            name = ann.getFile().getName()\n",
    "            # Select the ilatisk project TODO: use namespace\n",
    "            if name.endswith(\".ilp\"):\n",
    "                file_path = os.path.join(path, name)\n",
    "                with open(str(file_path), 'wb') as f:\n",
    "                    for chunk in ann.getFileInChunks():\n",
    "                        f.write(chunk)\n",
    "                file_paths.append(file_path)\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Helper function: load images labelled with a specific tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_tagged_images(dataset, tag):\n",
    "    images = dataset.listChildren()\n",
    "    tagged_images = list()\n",
    "    for image in images:\n",
    "        for ann in image.listAnnotations():\n",
    "            if ann.OMERO_TYPE == omero.model.TagAnnotationI and ann.getTextValue() == tag:\n",
    "                tagged_images.append(image)\n",
    "                break\n",
    "                \n",
    "    return tagged_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function: load an Image as 5D-numpy array: order TZYXC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def load_numpy_array(image):\n",
    "    image_shape = (image.getSizeT(),\n",
    "                   image.getSizeZ(),\n",
    "                   image.getSizeC(),\n",
    "                   image.getSizeX(),\n",
    "                   image.getSizeY())\n",
    "\n",
    "    nr_planes = image_shape[0] * image_shape[1] * image_shape[2]\n",
    "\n",
    "    zct_list = list(product(range(image_shape[1]),\n",
    "                            range(image_shape[2]),\n",
    "                            range(image_shape[0])))\n",
    "    pixels = image.getPrimaryPixels()\n",
    "    pixels_type = pixels.getPixelsType()\n",
    "    if pixels_type.value == 'float':\n",
    "        data_type = pixels_type.value + str(pixels_type.bitSize)  # TODO: Verify this is working for all data types\n",
    "    else:\n",
    "        data_type = pixels_type.value\n",
    "    stack = numpy.zeros((nr_planes,\n",
    "                      image.getSizeX(),\n",
    "                      image.getSizeY()), dtype=data_type)\n",
    "    numpy.stack(list(pixels.getPlanes(zct_list)), out=stack)\n",
    "    stack = numpy.reshape(stack, image_shape)\n",
    "\n",
    "    return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def plane_gen(data):\n",
    "    \"\"\"\n",
    "    Set up a generator of 2D numpy arrays.\n",
    "\n",
    "    The createImage method below expects planes in the order specified here\n",
    "    (for z.. for c.. for t..)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    for z in range(data.shape[0]):  # all Z sections data.shape[0]\n",
    "        for c in range(data.shape[1]):  # all channels\n",
    "            for t in range(data.shape[2]):  # all time-points\n",
    "                yield data[z][c][t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Export images to build an Ilastik model\n",
    "This section will get the images tagged as part of the training set and save them in a Ilastik-friendly format\n",
    "\n",
    "First we get the dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training image as /run/media/julio/DATA/Quentin/training_dataset/20181012_S2Rplus_FISH_DAPI_X1-A488_X2-A555_X3-A647_Mark_and_Find_001_Position001__DAPI.npy\n",
      "Saved training image as /run/media/julio/DATA/Quentin/training_dataset/20181012_S2Rplus_FISH_DAPI_X1-A488_X2-A555_X3-A647_Mark_and_Find_001_Position001__DAPI_ch1.npy\n",
      "Saved training image as /run/media/julio/DATA/Quentin/training_dataset/20181012_S2Rplus_FISH_DAPI_X1-A488_X2-A555_X3-A647_Mark_and_Find_001_Position001__DAPI_ch2.npy\n",
      "Saved training image as /run/media/julio/DATA/Quentin/training_dataset/20181012_S2Rplus_FISH_DAPI_X1-A488_X2-A555_X3-A647_Mark_and_Find_001_Position001__DAPI_ch3.npy\n",
      "Saved training image as /run/media/julio/DATA/Quentin/training_dataset/20181012_S2Rplus_FISH_DAPI_X1-A488_X2-A555_X3-A647_Mark_and_Find_001_Position002__DAPI.npy\n",
      "Saved training image as /run/media/julio/DATA/Quentin/training_dataset/20181012_S2Rplus_FISH_DAPI_X1-A488_X2-A555_X3-A647_Mark_and_Find_001_Position002__DAPI_ch1.npy\n",
      "Saved training image as /run/media/julio/DATA/Quentin/training_dataset/20181012_S2Rplus_FISH_DAPI_X1-A488_X2-A555_X3-A647_Mark_and_Find_001_Position002__DAPI_ch2.npy\n",
      "Saved training image as /run/media/julio/DATA/Quentin/training_dataset/20181012_S2Rplus_FISH_DAPI_X1-A488_X2-A555_X3-A647_Mark_and_Find_001_Position002__DAPI_ch3.npy\n",
      "Saved training image as /run/media/julio/DATA/Quentin/training_dataset/20181012_S2Rplus_FISH_DAPI_X1-A488_X2-A555_X3-A647_Mark_and_Find_001_Position003__DAPI.npy\n",
      "Saved training image as /run/media/julio/DATA/Quentin/training_dataset/20181012_S2Rplus_FISH_DAPI_X1-A488_X2-A555_X3-A647_Mark_and_Find_001_Position003__DAPI_ch1.npy\n",
      "Saved training image as /run/media/julio/DATA/Quentin/training_dataset/20181012_S2Rplus_FISH_DAPI_X1-A488_X2-A555_X3-A647_Mark_and_Find_001_Position003__DAPI_ch2.npy\n",
      "Saved training image as /run/media/julio/DATA/Quentin/training_dataset/20181012_S2Rplus_FISH_DAPI_X1-A488_X2-A555_X3-A647_Mark_and_Find_001_Position003__DAPI_ch3.npy\n"
     ]
    }
   ],
   "source": [
    "# Get the dataset to analyze\n",
    "dataset = conn.getObject('Dataset', dataset_id)\n",
    "print(dataset.getname())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "and then we can get the images associated to that dataset that are tagged as training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the training images\n",
    "training_images = get_tagged_images(dataset, training_dataset_tag)\n",
    "\n",
    "# Create a directory to save the images\n",
    "# training_path = tempfile.mkdtemp()\n",
    "training_path = '/run/media/julio/DATA/Quentin/training_dataset'\n",
    "if not os.path.exists(training_path):\n",
    "    os.makedirs(training_path)\n",
    "\n",
    "# Loop through every image to save it as a numpy array\n",
    "for image in training_images:\n",
    "    image_data = load_numpy_array(image)\n",
    "    filename, file_extension = os.path.splitext(image.getName())\n",
    "    filename = filename.replace('/', '_')\n",
    "    filename = filename.replace('.lif [', '_')\n",
    "    filename = filename.replace(']', '_')\n",
    "    training_file = os.path.join(training_path, filename)\n",
    "    numpy.save(f'{training_file}_DAPI', image_data[:,:,0,...])\n",
    "    print(f'Saved training image as {training_file}_DAPI.npy')\n",
    "\n",
    "    for ch in range(1, 4):\n",
    "        numpy.save(f'{training_file}_DAPI_ch{ch}', image_data[:,:,(0,ch),...])\n",
    "        print(f'Saved training image as {training_file}_DAPI_ch{ch}.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create an Ilastik model using the exported training images\n",
    "\n",
    "TODO: We should propose here to open the virtual desktop\n",
    "Instructions to train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Validate the models\n",
    "You should now have created four models: one to find the nucleus and one for each channel.\n",
    "These models should have been uploaded as attachments to the dataset so we can use them from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a temporary directory to save model and images\n",
    "validation_path = tempfile.mkdtemp()\n",
    "if not os.path.exists(validation_path):\n",
    "    os.makedirs(validation_path)\n",
    "    \n",
    "# Load the model linked to the dataset\n",
    "model_files = load_model(dataset_id, validation_path)\n",
    "\n",
    "# Load the images tagged for validation\n",
    "validation_images = get_tagged_images(dataset, validation_dataset_tag)\n",
    "\n",
    "# Create a new dataset where to upload the generated images\n",
    "validation_dataset = omero.model.DatasetI()\n",
    "v = f'ilastik_validation_probabilities_from_dataset_{dataset_id}'\n",
    "validation_dataset.setName(omero.rtypes.rstring(v))\n",
    "v = f'ilastik validation probabilities from dataset {dataset_id}. This dataset is for validation'\n",
    "validation_dataset.setDescription(omero.rtypes.rstring(v))\n",
    "validation_dataset = conn.getUpdateService().saveAndReturnObject(validation_dataset)\n",
    "\n",
    "# We link the new dataset to the same project as the original dataset\n",
    "source_project = dataset.getParent()\n",
    "link = omero.model.ProjectDatasetLinkI()\n",
    "link.setParent(source_project._obj)\n",
    "link.setChild(validation_dataset)\n",
    "conn.getUpdateService().saveObject(link)\n",
    "\n",
    "print('These are the models attached to the dataset')\n",
    "for m in model_files: print(m)\n",
    "print('These are the validation images')\n",
    "for i in validation_images: print(i.getName())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load each image as an 5D-numpy array and analyze.\n",
    "Now everything is setup we can go through the validation images and verify the results.\n",
    "We will have to specify which model we want to run for each channel. There are 4 channels:\n",
    "- DAPI = channel_0\n",
    "- A488 = channel_1\n",
    "- A555 = channel_2\n",
    "- A647 = channel_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z:236 t:1 c:2 y:275 x:271\n",
      "Downloading image B1_C1.tif\n",
      "Image converted\n",
      "running ilastik headless using /tmp/tmpInvF2D/pixel-class-wednesday.ilp on file B1_C1.tif\n",
      "Saving Probabilities as an Image in OMERO\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Specify here which model to use to segment each channel in the same order as they appear in the image\n",
    "model_files = [\n",
    "    'Nuclei_model_v2.ilp',\n",
    "    'Ch1_model_v2.ilp',\n",
    "    'Ch3_model_v2.ilp',\n",
    "    'Ch2_model_v2.ilp',\n",
    "    ]\n",
    "\n",
    "output_directory = tempfile.TemporaryDirectory()\n",
    "\n",
    "for image in validation_images:\n",
    "    filename, file_extension = os.path.splitext(image.getName())\n",
    "    filename = filename.replace('/', '_')\n",
    "    filename = filename.replace('.lif [', '_')\n",
    "    filename = filename.replace(']', '')\n",
    "    tmp_file = os.path.join(validation_path, filename)\n",
    "    image_data = load_numpy_array(image)\n",
    "    \n",
    "    for channel, model_file in enumerate(model_files):\n",
    "        if channel == 0:\n",
    "            tmp_file_channel = f'{tmp_file}_DAPI'\n",
    "            numpy.save(tmp_file_channel, image_data[:,:,0,...])\n",
    "        else:\n",
    "            tmp_file_channel = f'{tmp_file}_DAPI_Ch{channel}'\n",
    "            numpy.save(tmp_file_channel, image_data[:,:,(0,channel),...])\n",
    "\n",
    "        model_path = os.path.join(validation_path, model_file)\n",
    "        # run each ilastik model headless\n",
    "        print(f'running ilastik using {model_file} and {image.getName()}')\n",
    "        # save output in zctyx order expected by OMERO\n",
    "        cmd = ['/opt/python-apps/ilastik-release/run_ilastik.sh',\n",
    "               '--headless',\n",
    "               f'--project={model_path}',\n",
    "               '--export_source=Probabilities',\n",
    "               '--output_format=numpy',\n",
    "               f'--output_filename_format={{dataset_dir}}/{{nickname}}_Probabilities.npy',\n",
    "               #'--output_axis_order=zctyx', \n",
    "               f'{tmp_file_channel}.npy']\n",
    "        try:\n",
    "            subprocess.run(cmd, check=True, stdout=subprocess.PIPE).stdout\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f'Error: {e.output}')\n",
    "            print(f'Command: {e.cmd}')\n",
    "            print()\n",
    "        \n",
    "        print('List of outputs:')\n",
    "        for x in os.listdir('/home/jovyan/notebooks/DATA'):\n",
    "            print(x)\n",
    "    \n",
    "        # Save the probabilities file to the image\n",
    "        omero_name = os.path.splitext(os.path.basename(tmp_file_channel))[0] + '_Probabilities'\n",
    "        print(f'Saving Probabilities as an Image in OMERO as {omero_name}')\n",
    "        output_data = numpy.load(f'{tmp_file_channel}_Probabilities.npy')\n",
    "        print(f'old shape = {output_data.shape}')\n",
    "        if len(output_data.shape) == 4:\n",
    "            output_data = output_data.reshape(output_data.shape[:2] + (1,) + output_data.shape[2:])\n",
    "        print(f'new shape = {output_data.shape}')\n",
    "        desc = f'ilastik probabilities from Image {image.getId()} and model {model_file}'\n",
    "        conn.createImageFromNumpySeq(zctPlanes=plane_gen(output_data), \n",
    "                                     imageName=omero_name,\n",
    "                                     sizeZ=output_data.shape[0], \n",
    "                                     #sizeC=data.shape[2],\n",
    "                                     #sizeT=data.shape[0], \n",
    "                                     description=desc,\n",
    "                                     dataset=validation_dataset)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the connection to the OMERO server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### License\n",
    "Copyright (C) 2019 University of Dundee. All Rights Reserved.\n",
    "This program is free software; you can redistribute it and/or modify it\n",
    "under the terms of the GNU General Public License as published by the\n",
    "Free Software Foundation; either version 2 of the License, or\n",
    "(at your option) any later version.\n",
    "This program is distributed in the hope that it will be useful, but\n",
    "WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY\n",
    "or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for\n",
    "more details. You should have received a copy of the GNU General\n",
    "Public License along with this program; if not, write to the\n",
    "Free Software Foundation,\n",
    "Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}